---
title: "R Notebook"
output: pdf_document
---

```{r}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(broom))
suppressPackageStartupMessages(library(car))
```


- The core motivation: How do we describe the data

- There is a often a need to *dichotomize* a variable: 
e.g. we can use lm to associate number of cigarettes smoked per day against risk of lung cancer
but when we try to communicate the findings (to the public) then 

- this practical gives you a stronger ground to suggest that dichomizing a variable is a choice that can be considered with its own advantages (and obviously its limitations by throwing information away).

e.g. MEDIAN DICHOMIZATION of a continuous variable

- we are running a two-factor simulation: 
factor 1: a continuous IV --> run simulations
factor 2: a discrete IV   --> run simulations again
compare the results



# Exercise 1: Simulation with Continuous DV

## 1.1 Continuous DV and Continuous IVs (factor 1)

```{r}
# 1.1.1 set seed
set.seed(0)

# 1.1.2 Define simulation params
n <- 5
r <- 3 # number of monte carlo simulations
k <- 2  # number of predictors

Beta.sd <- 1 # sd for the slope coefficient (i.e. Beta)
```

```{r}
### 1.1.3 Init vector
### This vector is the result form each simulation
### For each simulation, we look 
nSignifIters <- vector(mode = "numeric", length = r)

### 1.1.4 Monte carlo main loop

for (simulation in 1:r) {
        
        print(paste("loop", simulation))
        
        ### 1.1.5 
        ### Generate the IVs: These should all be stored in a data.frame. 
        ### Each IV is a random vector of n observations, generated using rnorm.
        
        # init empty dataframe
        X <- matrix(0, nrow = n, ncol = k)
        
        # generate IV
        for (iv in 1:k) {
                X[,iv] <- rnorm(n)
        }
        
        print(X)
        
        ### 1.1.6
        ### Combine the IVs in a linear fashion, 
        ### with respect to a randomly-generated set of k coefficients, 
        ###   plus an error term.
        
        Beta <- rnorm(k, mean = 0, sd = Beta.sd)
        Y <- vector("numeric", n) 
        #! here we implicitly set the intercept to be zero,
        #  that's why later when we do a t.test,
        #  where H0 of intercept's estimate == 0,
        #  then the intercept should always be stat insignfiicant

        # effects of DV
        for (iv in 1:k) {
                Y <- Y + Beta[iv] * X[,iv]
        }
        
        Y2 <- rowSums(X * Beta) # automatically multiples each row with Beta
        waldo::compare(Y, Y2) %>% print
        # 
        # Epsilon <- rnorm(n)
        # Y <- Y + Epsilon # add Epsilon
        
        ### 1.1.7
        ### Combine the IVs and DV into a data frame, 
        ### and fit a linear model with respect to all the IVs.
        
        XY <- cbind(X, Y) %>% data.frame()
        Mod <- lm(Y ~ ., data = XY)
        
        ### 1.1.8
        ### Extract the number of predictors from this model that 
        ### have a p-value less or equal than 0.001, 
        ### and store this number into your container.
        
        Fit <- broom::tidy(Mod)
        nSignifIter <- Fit %>% 
                filter(p.value <= 0.001) %>% 
                nrow()
        
        
        nSignifIters[simulation] <- nSignifIter
}
```


```{r}
```



# Exercise 2

here we are looking at logistic regression

linear combination of predictors, THEN exponentiate it as a probability (?)

here the error term is generated by a Bernoulli function, not the rnorm function


